{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILn1s_4kNFTU"
      },
      "outputs": [],
      "source": [
        "# 安装genism\n",
        "!pip install genism"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 下载 Wikipedia 文件\n",
        "\n",
        "- [下载地址](https://dumps.wikimedia.org/enwiki/latest/)\n",
        "- 数据文件约10-20G，注意你的存储空间和下载时间\n",
        "- 你可以使用自己觉得合适大小的数据文件进行替换"
      ],
      "metadata": {
        "id": "sADwvFzjNt2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a corpus from Wikipedia dump file.\n",
        "Inspired by:\n",
        "https://github.com/panyang/Wikipedia_Word2vec/blob/master/v1/process_wiki.py\n",
        "文件存储为 make_wiki_corpus.py\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "def make_corpus(in_f, out_f): # 接收两个参数，输入文件名称和输出文件名称\n",
        "\n",
        "\t\"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n",
        "\n",
        "\toutput = open(out_f, 'w')\n",
        "\twiki = WikiCorpus(in_f) # 直接调用gensim的能力对wiki的xml文件进行解码\n",
        "\n",
        "\ti = 0\n",
        "\tfor text in wiki.get_texts():\n",
        "\t\toutput.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
        "\t\ti = i + 1\n",
        "\t\tif (i % 10000 == 0):\n",
        "\t\t\tprint('Processed ' + str(i) + ' articles')\n",
        "\toutput.close()\n",
        "\tprint('Processing complete!')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\tif len(sys.argv) != 3:\n",
        "\t\tprint('Usage: python make_wiki_corpus.py <wikipedia_dump_file> <processed_text_file>')\n",
        "\t\tsys.exit(1)\n",
        "\tin_f = sys.argv[1]\n",
        "\tout_f = sys.argv[2]\n",
        "\tmake_corpus(in_f, out_f)"
      ],
      "metadata": {
        "id": "pcKwhpipOeUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 使用命令行调用Python的方式调用代码\n",
        "- 调用时长基于你的电脑性能波动\n",
        "\n",
        "```bash\n",
        "python make_wiki_corpus enwiki-latest-pages-articles.xml.bz2 wiki_en.txt\n",
        "```"
      ],
      "metadata": {
        "id": "jMkX5l80Ojy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 对生成完成的文本簇文件进行检查\n",
        "\n",
        "- 基于巨大的文本量，即使是极为强大的个人电脑也很难直接将完成生成的文本簇载入内存中\n",
        "- 可以采用一次读取50行数据的方式，逐步载入数据并进行文本检查工作\n",
        "- 通过与命令行的交互，可以实现对文本信息的检查确认\n",
        "- 检查完文本后按下任何按钮继续检查后续50条文本\n",
        "- 检查完文本后键入`STOP`则会停职检查直接载入全部剩余文本"
      ],
      "metadata": {
        "id": "Jg7Sm0WqPxuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Checks a corpus created from a Wikipedia dump file.\n",
        "\"\"\"\n",
        "\n",
        "import sys, time\n",
        "\n",
        "def check_corpus(input_file):\n",
        "    \n",
        "    # 从文本簇中获取50条文本数据\n",
        "    # 不进行手动退出则不会主动退出\n",
        "    while(1):\n",
        "        for lines in range(50):\n",
        "            print(input_file.readline())\n",
        "        user_input = input('>>> Type \\'STOP\\' to quit or hit Enter key for more <<< ')\n",
        "        if user_input == 'STOP':\n",
        "            break\n",
        "\n",
        "\n",
        "def load_corpus(input_file):\n",
        "\n",
        "    \"\"\"Loads corpus from text file\"\"\"\n",
        "\n",
        "    print('Loading corpus...')\n",
        "    time1 = time.time()\n",
        "    corpus = input_file.read()\n",
        "    time2 = time.time()\n",
        "    total_time = time2-time1\n",
        "    print('It took %0.3f seconds to load corpus' %total_time)\n",
        "    return corpus\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    if len(sys.argv) != 2:\n",
        "        print('Usage: python check_wiki_corpus.py <corpus_file>')\n",
        "        sys.exit(1)\n",
        "\n",
        "    corpus_file = open(sys.argv[1],'r')\n",
        "    check_corpus(corpus_file)\n",
        "    corpus = load_corpus(corpus_file)"
      ],
      "metadata": {
        "id": "Gn_w2BGhOp4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 使用命令行的方式调用函数\n",
        "\n",
        "```bash\n",
        "python check_wiki_corpus.py wiki_en.txt\n",
        "```"
      ],
      "metadata": {
        "id": "Xw6IYETqRMOU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TSLQyMrRR6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}