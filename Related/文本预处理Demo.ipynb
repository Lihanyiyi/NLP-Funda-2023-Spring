{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP 文本预处理\n",
        "\n",
        "- 本Notebook旨在提供一个base思路介绍NLP中对于文本预处理操作的步骤\n",
        "- 本文中的思路不代表最佳实践"
      ],
      "metadata": {
        "id": "LcbjWTleT3_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s68U9ArEVFlV",
        "outputId": "f3a76850-5200-4f8e-ccf0-56dc0dadf2ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M9t5olbTe1X",
        "outputId": "5cf8e7e8-6dda-46e1-a930-72ac607f60a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# 引入需要的包\n",
        "import re, string, unicodedata\n",
        "import nltk # NLP Toolkit，包括 tokenization，stemming 和词性标注等能力\n",
        "import contractions # 扩展英文缩写为全表达\n",
        "import inflect # 生成复数，单数名词，序数词，不定冠词等\n",
        "from bs4 import BeautifulSoup # 用于处理HTML或是XML结构的数据\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 设定一段文本作为样本文本簇\n",
        "\n",
        "- 你可以选择其他文本作为训练样本"
      ],
      "metadata": {
        "id": "cTdia643U9WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"\"\"<h1>Title Goes Here</h1>\n",
        "<b>Bolded Text</b>\n",
        "<i>Italicized Text</i>\n",
        "<img src=\"this should all be gone\"/>\n",
        "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
        "I run. He ran. She is running. Will they stop running?\n",
        "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
        "[Some text we don't want to keep is in here]\n",
        "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
        "something... is! wrong() with.,; this :: sentence.\n",
        "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
        "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
        "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
        "[This is some other unwanted text]\n",
        "John: \"Well, well, well.\"\n",
        "James: \"There, there. There, there.\"\n",
        "&nbsp;&nbsp;\n",
        "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
        "I have to go get 2 tutus from 2 different stores, too.\n",
        "22    45   1067   445\n",
        "{{Here is some stuff inside of double curly braces.}}\n",
        "{Here is more stuff in single curly braces.}\n",
        "[DELETE]\n",
        "</body>\n",
        "</html>\"\"\""
      ],
      "metadata": {
        "id": "a5wVcmDZU8tg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9LaMahzWVvL",
        "outputId": "0f77eab4-460e-4b03-9376-1eef1c10e25b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title Goes Here\n",
            "Bolded Text\n",
            "Italicized Text\n",
            "\n",
            "But this will still be here!\n",
            "I run. He ran. She is running. Will they stop running?\n",
            "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
            "\n",
            "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
            "something... is! wrong() with.,; this :: sentence.\n",
            "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
            "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
            "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
            "\n",
            "John: \"Well, well, well.\"\n",
            "James: \"There, there. There, there.\"\n",
            "  \n",
            "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
            "I have to go get 2 tutus from 2 different stores, too.\n",
            "22    45   1067   445\n",
            "{{Here is some stuff inside of double curly braces.}}\n",
            "{Here is more stuff in single curly braces.}\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise Removal 移除文本噪音\n",
        "\n",
        "- 移除文本文件中的头和尾\n",
        "- 移除 HTML XML 等结构化标签数据\n",
        "- 对JSON结构的数据进行抽取\n",
        "- 大多数时候通过 `BeautifulSoup` 和 `Regex` 实现"
      ],
      "metadata": {
        "id": "FwmHr9AVVlxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text): # 移除中括号\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "sample = denoise_text(sample)\n",
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__oSupsbU7EM",
        "outputId": "e2a1219e-56ea-4f50-cc6c-cdfdf0c980a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title Goes Here\n",
            "Bolded Text\n",
            "Italicized Text\n",
            "\n",
            "But this will still be here!\n",
            "I run. He ran. She is running. Will they stop running?\n",
            "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
            "\n",
            "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
            "something... is! wrong() with.,; this :: sentence.\n",
            "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
            "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
            "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
            "\n",
            "John: \"Well, well, well.\"\n",
            "James: \"There, there. There, there.\"\n",
            "  \n",
            "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
            "I have to go get 2 tutus from 2 different stores, too.\n",
            "22    45   1067   445\n",
            "{{Here is some stuff inside of double curly braces.}}\n",
            "{Here is more stuff in single curly braces.}\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 为了避免在tokenization 阶段，将 `didn't` 切分为 `did` 和 `n't` 两个字符，需要对文本进行扩充操作"
      ],
      "metadata": {
        "id": "MUobIzOaWirt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_contractions(text):\n",
        "    # 将缩写转化为非缩写\n",
        "    return contractions.fix(text)\n",
        "\n",
        "sample = replace_contractions(sample)\n",
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StmK2CABWTv-",
        "outputId": "e86615cd-bc09-4aee-fe90-73456c508bd6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title Goes Here\n",
            "Bolded Text\n",
            "Italicized Text\n",
            "\n",
            "But this will still be here!\n",
            "I run. He ran. She is running. Will they stop running?\n",
            "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
            "\n",
            "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
            "something... is! wrong() with.,; this :: sentence.\n",
            "I cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\n",
            "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
            "Do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\n",
            "\n",
            "John: \"Well, well, well.\"\n",
            "James: \"There, there. There, there.\"\n",
            "  \n",
            "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
            "I have to go get 2 tutus from 2 different stores, too.\n",
            "22    45   1067   445\n",
            "{{Here is some stuff inside of double curly braces.}}\n",
            "{Here is more stuff in single curly braces.}\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "- 将文本句子拆分成更小的部分，token，颗粒度大小为 word\n"
      ],
      "metadata": {
        "id": "Y0r8hvgyXNti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(sample)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha2CRS3IW3k4",
        "outputId": "ed66a501-c7eb-41fb-af19-a8e1a1bfd697"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'this', 'will', 'still', 'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', 'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', 'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '¡Sebastián', ',', 'Nicolás', ',', 'Alejandro', 'and', 'Jéronimo', 'are', 'going', 'to', 'the', 'store', 'tomorrow', 'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'Do', 'not', 'do', 'it', '....', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', 'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', 'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', '.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', 'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', '.', '}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization 标准化\n",
        "\n",
        "- stemming\n",
        "- lemmatization\n",
        "- others\n",
        "  - 移除非ASCII\n",
        "  - 全部小写化\n",
        "  - 移除标点符号\n",
        "  - 替换数字为对应的单词\n",
        "  - 移除停用词\n"
      ],
      "metadata": {
        "id": "uFeEVfhgXqUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_ascii(words):\n",
        "    # 移除非ASCII的字符\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    # 全部小写化\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    # 移除标点符号\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def replace_numbers(words):\n",
        "    # 替换数字为对应的单词\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    # 移除停用词\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def stem_words(words):\n",
        "    # 对单词进行词干提取\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    # 对动词进行词性还原\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n",
        "\n",
        "words = normalize(words)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC0hWEiHXeur",
        "outputId": "6bb24b30-b18b-404c-a02b-f8aa1504d8db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['title', 'goes', 'bolded', 'text', 'italicized', 'text', 'still', 'run', 'ran', 'running', 'stop', 'running', 'talked', 'talking', 'talked', 'running', 'ran', 'talking', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 'going', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchises', 'order', 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'wars', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 'little', 'house', 'got', 'john', 'well', 'well', 'well', 'james', 'lot', 'reasons', 'one hundred and one', 'reasons', 'one million', 'reasons', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'stores', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'braces', 'stuff', 'single', 'curly', 'braces']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_and_lemmatize(words):\n",
        "    stems = stem_words(words)\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return stems, lemmas\n",
        "\n",
        "stems, lemmas = stem_and_lemmatize(words)\n",
        "print('Stemmed:\\n', stems) # 次干提取的文本\n",
        "print('\\nLemmatized:\\n', lemmas) # 词性还原的文本"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-6z7YzkY08N",
        "outputId": "88072185-0247-493e-f12e-113348962a4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed:\n",
            " ['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', 'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', 'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', 'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', 'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', 'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']\n",
            "\n",
            "Lemmatized:\n",
            " ['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', 'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future\n",
        "\n",
        "- 完成上述的三步操作后，就可以使用最后生成的文本进行下游任务了\n",
        "- 基于下游任务的性质，选择 `Stemming` 或 `Lemmatization`处理后的token"
      ],
      "metadata": {
        "id": "WyQO7eYvZcyu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gky1vKejY-lS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}