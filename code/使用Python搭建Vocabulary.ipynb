{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 使用Python构建NLP预训练基础-Vocabulary\n",
        "\n",
        "在处理NLP任务时，通常来说，文本数据预的处理流程大致如下：\n",
        "\n",
        "1. 获取原始文本簇 Corpus\n",
        "2. 处理文本\n",
        "3. 将文本标签化处理\n",
        "4. 生成文本簇字典 Vocabulary\n",
        "5. 处理文本表示\n",
        "\n",
        "而构建字典的过程是在文本表示和下游任务之前必须进行的操作\n",
        "\n",
        "构建字典的过程并不仅仅是将文本簇中的文字以Set的方式存储，还可以存储文本簇中的元数据信息\n"
      ],
      "metadata": {
        "id": "lyhiKnLLo--1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第一步\n",
        "\n",
        "创建用于划定文本范围的字符\n",
        "\n",
        "1. start of sentence 标识句子的开头\n",
        "2. end of sentence 标识句子的结尾\n",
        "3. padding of sentence 填充长度较短的文本"
      ],
      "metadata": {
        "id": "4OdW96loqR-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u5tWI7Nko8FU"
      },
      "outputs": [],
      "source": [
        "PAD_token = 0   # Used for padding short sentences\n",
        "SOS_token = 1   # Start-of-sentence token\n",
        "EOS_token = 2   # End-of-sentence token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第二步\n",
        "\n",
        "- 创建一个 Vocabulary 类的构造函数\n",
        "- 存储文本簇的元数据信息"
      ],
      "metadata": {
        "id": "QJsbb3bLrGX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, name):\n",
        "  self.name = name\n",
        "  self.word2index = {} # 单词转下标的字典\n",
        "  self.word2count = {} # 单词计数的字典\n",
        "  self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} # 下标转单词的字典\n",
        "  self.num_words = 3 # 字典中的单词总数\n",
        "  self.num_sentences = 0 # 文本簇中的句子总数\n",
        "  self.longest_sentence = 0 # 文本簇中最长的句子"
      ],
      "metadata": {
        "id": "arfV-8cdrFqn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第三步\n",
        "\n",
        "- 创建填充字典的函数，以单词为维度"
      ],
      "metadata": {
        "id": "lfDA10tBsZ6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_word(self, word): # 以单词为输入维度，将单词送入字典中存储\n",
        "  # 基于字典已经存储过当前单词和未存储过当前单词作为分界\n",
        "  # 按照单词送入类的顺序进行下标计数，没有特殊的处理步骤\n",
        "  if word not in self.word2index:\n",
        "    # First entry of word into vocabulary\n",
        "    self.word2index[word] = self.num_words\n",
        "    self.word2count[word] = 1\n",
        "    self.index2word[self.num_words] = word\n",
        "    self.num_words += 1\n",
        "  else:\n",
        "    # 当单词已经存在于字典中时，直接对计数+1\n",
        "    self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "UD5qsuv-q_tx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 创建填充字典的函数，以句子为维度，使用刚刚创建的单词创建函数\n",
        "- 这里对于文本的处理仅有，使用空格进行分词的操作，是偷懒的做法，真实的场景会更加复杂"
      ],
      "metadata": {
        "id": "L3Sh_by-tAq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sentence(self, sentence): # 以句子为输入维度\n",
        "  sentence_len = 0\n",
        "  for word in sentence.split(' '):\n",
        "    sentence_len += 1\n",
        "    self.add_word(word)\n",
        "  if sentence_len > self.longest_sentence:\n",
        "    # 每一次输入的新句子，若长度超过之前最长的句子，则更新最长句子的原信息\n",
        "    self.longest_sentence = sentence_len\n",
        "  # 计算文本簇中句子的数量\n",
        "  self.num_sentences += 1"
      ],
      "metadata": {
        "id": "fDCcSphdslk8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 创建两个查找函数，扩充字典类的基本查询功能\n",
        "\n",
        "1. 基于单词查询下标\n",
        "2. 基于下标查询单词"
      ],
      "metadata": {
        "id": "hUqNDZB1t58s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 将上述内容串联成 Vocabulary 类"
      ],
      "metadata": {
        "id": "iLpxPiecuM-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    PAD_token = 0\n",
        "    SOS_token = 1   \n",
        "    EOS_token = 2   \n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "        self.num_sentences = 0\n",
        "        self.longest_sentence = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "    def add_sentence(self, sentence):\n",
        "        sentence_len = 0\n",
        "        for word in sentence.split(' '):\n",
        "            sentence_len += 1\n",
        "            self.add_word(word)\n",
        "        if sentence_len > self.longest_sentence:\n",
        "            self.longest_sentence = sentence_len\n",
        "        self.num_sentences += 1\n",
        "\n",
        "    def to_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def to_index(self, word):\n",
        "        return self.word2index[word]"
      ],
      "metadata": {
        "id": "pztkxLObuQSL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用简单文本构造一个字典"
      ],
      "metadata": {
        "id": "aPVZdqdIujqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voc = Vocabulary('test')\n",
        "print(voc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecjXgVpft5y5",
        "outputId": "be7e063b-a085-448d-abc0-b71fe312c755"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Vocabulary object at 0x7ff4f81d2100>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 假定一个简单的文本簇\n",
        "corpus = ['This is the first sentence.',\n",
        "          'This is the second.',\n",
        "          'There is no sentence in this corpus longer than this one.',\n",
        "          'I love China.']\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRgEhx0fuqx1",
        "outputId": "8250e785-d801-4305-cabe-935882c8b976"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.', 'This is the second.', 'There is no sentence in this corpus longer than this one.', 'I love China.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in corpus:\n",
        "  voc.add_sentence(sent)\n",
        "\n",
        "print('Token 4 corresponds to token:', voc.to_word(4))\n",
        "print('Token \"this\" corresponds to index:', voc.to_index('this'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ0P3gPptcz_",
        "outputId": "823bd56c-98a0-42b7-cadc-65d7bbee5267"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 4 corresponds to token: is\n",
            "Token \"this\" corresponds to index: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 遍历整个字典\n",
        "\n",
        "for word in range(voc.num_words):\n",
        "    print(voc.to_word(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apkcbBzPvExF",
        "outputId": "2d7bdb2e-5358-4fdb-da2f-9640a3e2b82d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PAD\n",
            "SOS\n",
            "EOS\n",
            "This\n",
            "is\n",
            "the\n",
            "first\n",
            "sentence.\n",
            "second.\n",
            "There\n",
            "no\n",
            "sentence\n",
            "in\n",
            "this\n",
            "corpus\n",
            "longer\n",
            "than\n",
            "one.\n",
            "I\n",
            "love\n",
            "China.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将文本簇中的第三个句子，转化为字典index的形式\n",
        "# 在转化之前，我们需要对句子的开头结尾加上SOS和EOS\n",
        "\n",
        "sent_tkns = []\n",
        "sent_idxs = []\n",
        "for word in corpus[3].split(' '):\n",
        "  sent_tkns.append(word)\n",
        "  sent_idxs.append(voc.to_index(word))\n",
        "print(sent_tkns)\n",
        "print(sent_idxs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJvKNwaxvLWq",
        "outputId": "e1e3791b-9920-4e45-db0f-7c4b63bd0412"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'China.']\n",
            "[18, 19, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 虽然这样一个最base的 Vocabulary 已经实现了，但是细节上仍然还有很多缺陷，比如：\n",
        "\n",
        "1. 没有对文本进行标准化处理（大小写归一，停用词，标点符号）\n",
        "2. 虽然使用了 SOS 和 EOS 但是没有对句子的长度进行规划，没有使用 padding symbol\n",
        "3. 没有对字典表进行修剪（对于出现频率极少的词进行适当的剔除操作，降低字典表的大小）"
      ],
      "metadata": {
        "id": "P8XnAwCOvvDc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-mh4TVPvg4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}